{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLA+9xWtSahQfh6Yr5MBME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iewandowski/LP_Tokenizer/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "g0ZQSmEYnU0_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpMWhUDuDy1P",
        "outputId": "f5219c34-df08-4d08-fbd7-e08881b37e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( a , ['IDENT', 1] )\n",
            "(' := ', ['ASSIGN_OP', 12] )\n",
            "( = , ['IDENT', 1] )\n",
            "(' ( ', ['LPAREN', 3] )\n",
            "( aux , ['IDENT', 1] )\n",
            "(' - ', ['SUB_OP', 6] )\n",
            "( 2 , ['INT_LIT', 2] )\n",
            "(  , ['IDENT', 1] )\n",
            "(' ) ', ['RPAREN', 4] )\n",
            "(' * ', ['MUL_OP', 7] )\n",
            "( 200 , ['INT_LIT', 2] )\n",
            "(' / ', ['DIV_OP', 8] )\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "list = []\n",
        "values = \"\"\n",
        "others = {'vars': [\"IDENT\", 1],\n",
        "          'num': [\"INT_LIT\", 2]\n",
        "          }\n",
        "\n",
        "\n",
        "tokens = {\"(\": [\"LPAREN\", 3], \n",
        "          \")\": [\"RPAREN\", 4], \n",
        "          \"+\": [\"ADD_OP\", 5], \n",
        "          \"-\": [\"SUB_OP\", 6], \n",
        "          \"*\": [\"MUL_OP\", 7], \n",
        "          \"/\": [\"DIV_OP\", 8], \n",
        "          \">\": [\"GT_OP\", 9],\n",
        "          \"<\": [\"LT_OP\", 10], \n",
        "          \"==\": [\"EQ_OP\", 11], \n",
        "          \":=\": [\"ASSIGN_OP\", 12]\n",
        "          }\n",
        "\n",
        "with open('line.txt') as file:\n",
        "  for linha in file:\n",
        "     txt = linha\n",
        "     texto = txt.replace(\" \", \"\")\n",
        "  for word in texto:\n",
        "    list.append(word)\n",
        "\n",
        "  for i in range(len(list)):\n",
        "    this_next = \"\"    \n",
        "    if i + 1 < len(list):\n",
        "      next = list[i + 1]\n",
        "      this_next = list[i] + list[i+1]\n",
        "    if i + 2 < len(list):\n",
        "      next_2 = list[i+1] + list[i+2]\n",
        "    if i != 0:\n",
        "      last = list[i - 1] + list[i]\n",
        "    if list[i] not in tokens.keys() and this_next not in tokens.keys():\n",
        "      values = values + list[i]\n",
        "    if next in tokens.keys() or next_2 in tokens.keys():\n",
        "      if values.isnumeric():\n",
        "        print(\"(\", values, \",\", others.get('num'), \")\")\n",
        "      elif values.isalpha:\n",
        "        print(\"(\", values, \",\", others.get('vars'), \")\")\n",
        "      values = \"\"\n",
        "    if list[i] in tokens.keys():\n",
        "      print(\"('\", list[i], \"',\", tokens.get(str(list[i])), \")\")\n",
        "    if this_next in tokens.keys():\n",
        "      print(\"('\", this_next, \"',\", tokens.get(str(this_next)), \")\")"
      ]
    }
  ]
}