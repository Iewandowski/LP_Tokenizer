{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNx2P6a2MqHo92UM/2veShI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iewandowski/LP_Tokenizer/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "g0ZQSmEYnU0_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpMWhUDuDy1P",
        "outputId": "6e63a7d3-ba93-4b35-f25a-124daa280aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( a , ['IDENT', 1] )\n",
            "(' := ', ['ASSIGN_OP', 12] )\n",
            "(' ( ', ['LPAREN', 3] )\n",
            "( aux , ['IDENT', 1] )\n",
            "(' - ', ['SUB_OP', 6] )\n",
            "( 2 , ['INT_LIT', 2] )\n",
            "(' ) ', ['RPAREN', 4] )\n",
            "(' * ', ['MUL_OP', 7] )\n",
            "( 200 , ['INT_LIT', 2] )\n",
            "(' / ', ['DIV_OP', 8] )\n",
            "( 19 , ['INT_LIT', 2] )\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "list = []\n",
        "var = \"\"\n",
        "others = {'vars': [\"IDENT\", 1],\n",
        "          'num': [\"INT_LIT\", 2]\n",
        "          }\n",
        "\n",
        "\n",
        "tokens = {\"(\": [\"LPAREN\", 3], \n",
        "          \")\": [\"RPAREN\", 4], \n",
        "          \"+\": [\"ADD_OP\", 5], \n",
        "          \"-\": [\"SUB_OP\", 6], \n",
        "          \"*\": [\"MUL_OP\", 7], \n",
        "          \"/\": [\"DIV_OP\", 8], \n",
        "          \">\": [\"GT_OP\", 9],\n",
        "          \"<\": [\"LT_OP\", 10], \n",
        "          \"==\": [\"EQ_OP\", 11], \n",
        "          \":=\": [\"ASSIGN_OP\", 12],\n",
        "          \"=\": [\"EQ_OP\", 13]\n",
        "          }\n",
        "\n",
        "with open('line.txt') as file:\n",
        "  for linha in file:\n",
        "     txt = linha\n",
        "     texto = txt.replace(\" \", \"\")\n",
        "  for word in texto:\n",
        "    list.append(word)\n",
        "  for i in range(len(list)):\n",
        "    token = tokens.keys()\n",
        "    current = list[i]\n",
        "    if i == 0:\n",
        "      last = list[i]\n",
        "    else:\n",
        "      last = list[i - 1] + list[i]\n",
        "    if i == len(list) - 1:      \n",
        "      next = list[i]\n",
        "      current_next = list[i]\n",
        "    elif i == len(list) - 2:\n",
        "      both_next = list[i]\n",
        "    else:\n",
        "      next = list[i + 1]\n",
        "      current_next = list[i] + list[i + 1]\n",
        "      both_next = list[i + 1] + list[i + 2]\n",
        "    if current not in token and last not in token and current_next not in token:\n",
        "      var = var + current\n",
        "    if next in token or both_next in token or current_next in token or i == len(list) - 1:\n",
        "      if var != \"\":\n",
        "        if var.isnumeric():\n",
        "          print(\"(\", var, \",\", others.get('num'), \")\")\n",
        "        elif var.isalpha:\n",
        "          print(\"(\", var, \",\", others.get('vars'), \")\")\n",
        "        var = \"\"\n",
        "    if current in token and last not in token:\n",
        "          print(\"('\", list[i], \"',\", tokens.get(str(list[i])), \")\")\n",
        "    if current_next in token:\n",
        "          print(\"('\", current_next, \"',\", tokens.get(str(current_next)), \")\")\n"
      ]
    }
  ]
}