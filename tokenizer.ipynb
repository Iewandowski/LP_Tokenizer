{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9Guu7VGGa9Vvv5US+NsAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iewandowski/LP_Tokenizer/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "g0ZQSmEYnU0_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpMWhUDuDy1P",
        "outputId": "27463935-44ee-4734-c277-240a3a5e01d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:=(aux-2)*200/19\n",
            "( a , ['IDENT', 1] )\n",
            "(' := ', ['ASSIGN_OP', 12] )\n",
            "( = , ['IDENT', 1] )\n",
            "(' ( ', ['LPAREN', 3] )\n",
            "( aux , ['IDENT', 1] )\n",
            "(' - ', ['SUB_OP', 6] )\n",
            "( 2 , ['INT_LIT', 2] )\n",
            "(  , ['IDENT', 1] )\n",
            "(' ) ', ['RPAREN', 4] )\n",
            "(' * ', ['MUL_OP', 7] )\n",
            "( 200 , ['INT_LIT', 2] )\n",
            "(' / ', ['DIV_OP', 8] )\n"
          ]
        }
      ],
      "source": [
        "texto = []\n",
        "\n",
        "others = {'vars': [\"IDENT\", 1],\n",
        "          'num': [\"INT_LIT\", 2]\n",
        "          }\n",
        "\n",
        "\n",
        "tokens = {\"(\": [\"LPAREN\", 3], \n",
        "          \")\": [\"RPAREN\", 4], \n",
        "          \"+\": [\"ADD_OP\", 5], \n",
        "          \"-\": [\"SUB_OP\", 6], \n",
        "          \"*\": [\"MUL_OP\", 7], \n",
        "          \"/\": [\"DIV_OP\", 8], \n",
        "          \">\": [\"GT_OP\", 9],\n",
        "          \"<\": [\"LT_OP\", 10], \n",
        "          \"==\": [\"EQ_OP\", 11], \n",
        "          \":=\": [\"ASSIGN_OP\", 12]\n",
        "          }\n",
        "\n",
        "with open('line.txt') as file:\n",
        "  for linha in file:\n",
        "     txt = linha\n",
        "     texto = txt.replace(\" \", \"\")\n",
        "  print(texto)\n",
        "  for ch1, ch2, ch3 in zip(texto, texto[1:], texto[2:]):\n",
        "    last_char = \n",
        "    double_char = ch1 + ch2\n",
        "    next_double_char = ch2 + ch3\n",
        "    if ch1 not in tokens.keys() and double_char not in tokens.keys():\n",
        "      vars = vars + ch1\n",
        "    if ch2 in tokens.keys() or next_double_char in tokens.keys():\n",
        "      if ch1.isnumeric():\n",
        "        print(\"(\", vars, \",\", others.get('num'), \")\")\n",
        "      elif ch1.isalpha:\n",
        "        print(\"(\", vars, \",\", others.get('vars'), \")\")\n",
        "      vars = \"\"\n",
        "    if ch1 in tokens.keys():\n",
        "      print(\"('\", ch1, \"',\", tokens.get(str(ch1)), \")\")\n",
        "    if double_char in tokens.keys():\n",
        "      print(\"('\", double_char, \"',\", tokens.get(str(double_char)), \")\")"
      ]
    }
  ]
}